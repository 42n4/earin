%lualatex --shell-escape  <name>.tex; 
%view <name>.pdf
%\documentclass[smaller, proffesionalfonts]{beamer}
\documentclass[proffesionalfonts]{beamer}
\usecolortheme[named=gray]{structure}
\mode<presentation> {
%\usetheme{Madrid} % My favorite!
%\usetheme{Boadilla} % Tretty neat, soft color.
%\usetheme{default}
\usetheme{Warsaw}
%\usetheme{Bergen} % This template has nagivation on the left
%\usetheme{Frankfurt} % Similar to the default with an extra region at the top.
%\usecolortheme{seahorse} % Simple and clean template
%\usetheme{Darmstadt} % not so good
% Uncomment the following line if you want page numbers and using Warsaw theme
% \setbeamertemplate{footline}[page number]
\setbeamercovered{transparent}
%\setbeamercovered{invisible}
% To remove the navigation symbols from the bottom of slides%
\setbeamertemplate{navigation symbols}{} 
}

\usepackage{ifpdf}
\usepackage{ifluatex}
\ifluatex
\usepackage{polyglossia}   
\setmainlanguage{polish}
\usepackage{amsmath}
\usepackage{mathtools}
\def\bm{\boldsymbol}
\usepackage{lualatex-math}
\usepackage{fontspec}
\usepackage{fancyvrb}
\else
\usepackage{graphicx}
\usepackage{polski} 
\usepackage[utf8x]{inputenc}
\usepackage{bm} 
\fi

\usepackage{ucs}
\usepackage{listingsutf8}
\usepackage{hyperref}
\usepackage{ragged2e}

\usepackage{pgf,tikz}
\usetikzlibrary{scopes,matrix,chains,positioning,graphs,graphdrawing,shapes,automata,calc,arrows.meta,quotes,shadows,intersections,fadings,decorations.pathreplacing}
\usepackage{cfr-lm}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{tkz-euclide}
%\usepackage{palatino}               %font definition

\tikzset{
  mymx/.style={matrix of math nodes,nodes=myball,column sep=10mm,row sep=7mm},
  myball/.style={draw,circle,inner sep=0pt},
  mylabel/.style={midway,sloped,fill=white,inner sep=1pt,outer sep=1pt,below,
    execute at begin node={$\scriptstyle},execute at end node={$}},
  plain/.style={draw=none,fill=none},
  sel/.append style={fill=green!10},
  psel/.append style={fill=red!10},
  route/.style={-latex,thick},
  selroute/.style={route,blue!50!green}
}

\DeclareMathOperator{\sgn}{sign}
%\usegdlibrary{trees}
%\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.
%\apptocmd{\block}{}{\justifying}{} % Allow optional arguments after frame.
%\newcommand{\mycircle}[1]{\tikz{\filldraw[draw=#1,fill=#1] (0,0) circle [radius=0.3em];}}

\title[EARIN]{Data Mining Lectures - Neural Networks}

\author{Piotr Wasiewicz}
\institute[ICS PW]
{
Institute of Computer Science\\
\medskip
{\emph{pwasiewi@elka.pw.edu.pl}}
}
\date{\today}
% \today will show current date. 
% Alternatively, you can specify a date.

\begin{document}
\lstset{language=R,literate={<-}{{$\gets$}}1}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Neural Networks: Neuron}
\begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2] 
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex] 
  {$w_2$};
\node[on chain=2,init] (sigma) 
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activate \\ function}}]   
  {$f$};
\node[on chain=2,label=above:Output,join=by -latex] 
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm) 
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex] 
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm) 
  (x3) {$x_3$};
\node[on chain=3,label=below:Weights,join=by o-latex] 
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
\end{tikzpicture}
\begin{block}{}
\begin{itemize}
\item[\ ] \[y=f(\langle\mathbf w\mathbf x\rangle + b)=f(\sum_{i\in\{1,2,3\}} w_ix_i + b)\]
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{Neural Networks: Sigmoid function}
\begin{tikzpicture} 
\begin{axis}[ xlabel=$\langle\mathbf w\mathbf x\rangle + b$, ylabel={$f(\langle\mathbf w\mathbf x\rangle + b)$}, 
xmin=-5, xmax=5, ymin=-0.15, ymax=1.15, legend pos=outer north east]
\addlegendimage{green, mark=x}
\addlegendimage{blue, mark=o}
\addplot[color=black,mark=-] coordinates { (0,-0.15) (0,1.15) }; 
\addplot[color=green,mark=x] {1/(1+e^(-x))}; 
\addlegendentry{$\frac{1}{1+e^{-(\langle\mathbf w\mathbf x\rangle + b)}}$}
\addplot[color=green,mark=x] {1/(1+e^(-x))}; 
\addplot[blue, mark=o][domain=-5:0]{0};
\addplot[blue, mark=o] coordinates { (0,0) (0,1) }; 
\addplot[blue, mark=o][domain=0:5]{1};
\addlegendentry{$perceptron$}
\end{axis} 
\end{tikzpicture} 
\begin{block}{}
\begin{itemize}
\item[\ ] The sigmoid function $f(a)$ and its derivative $f'(a)=f(a)*(1-f(a))$ has the main role in error backpropagation.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Neural Networks: neuron layers}
\begin{tikzpicture}
  \matrix[mymx] (mx) {
 %          1           &         2        &    3   			&   4     			&   5     			\\  
     |[plain]|   A_1	&[2cm]             & |[plain]| A_2 		&	             	& 	|[plain]| h=A_3	\\[-8mm] %1
	 |[plain]|   b_1  	& |[psel]| \sum    & |[sel]| f(n_{21}) 	&   |[plain]| b_2 						\\ %2
	 |[plain]|   x_1	&             \sum & f(n_{22}) 			&   \sum  			& f(n_{31})			\\ %3       
     |[plain]|   x_2    & 			  \sum & f(n_{23})				    								\\ %4
};
  {[route]
    \foreach \y in {2,3,4} {
      \draw (mx-\y-1) -- (mx-2-2);
      \draw (mx-\y-1) -- (mx-3-2);
      \draw (mx-\y-1) -- (mx-4-2);
       }
    \foreach \y in {2,3,4} {
      \draw (mx-\y-2) -- (mx-\y-3); }
    \draw (mx-2-4) -- (mx-3-4); %b_2
    \foreach \y in {2,3,4} {
    \draw (mx-\y-3) -- (mx-3-4);}
    \draw (mx-3-4) -- (mx-3-5);
  }
  {[selroute]
    \draw (mx-3-1) -- (mx-2-2) node[mylabel,above] { w_{(x_1)1} };
    \draw (mx-4-1) -- (mx-2-2) node[mylabel] { w_{(x_2)1} };
  }
  \node[above right = 2mm and 11mm of mx-2-1]  {$W_1$};
  \node[above right = 0cm and 2mm of mx-2-3]  {$W_2$};
  \node[above right = 2mm and 0mm of mx-2-2]  {$N_2$};
  \node[above right = 2mm and 0mm of mx-2-4]  {$N_3$};
\end{tikzpicture}
\begin{block}{}
\begin{itemize}
\item[\ ] $\displaystyle a_{21} = f(n_{21}) = f (w_{(x_1)1} x_1 + w_{(x_2)1} x_2 + b_{11}) $
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{Approximation Error}
\begin{block}{Mean Squared Error}
\begin{itemize}
\item[\ ]
\begin{align*}
E_{\text{total}}=\sum_{p\in T}E_{MSE}^p\ \ \  \text{where } p & \text{ is an element in a training set }T\\
E_{MSE}^h		=\frac{1}{2} \sum_{j \in M} \left(\hat{y}_h - h \right)^2&\\
\end{align*}
\end{itemize}
\end{block}
\begin{block}{Backpropagation of error to weights}
\begin{itemize}
\item[\ ]
\[ \vartriangle w_{mh} \sim - \nabla_w \cdot \ E_{MSE}^h\] where $w_{mh}$ is the connection weight from the neuron $m$ to the neuron $h$. \\
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Error backpropagation to the output neuron weights}
\begin{block}{The gradient of MSE influences the weights of connections to the output neuron}
\begin{itemize}
\item[\ ]
\[ \vartriangle w_{mh} = - \eta \textcolor{red}{\frac{\partial E \left( w_{mh} \right) }{ \partial w_{mh}} } \]  where $\eta$ is the training coefficient.
\end{itemize}
\end{block}
\begin{block}{The backpropagation algorithm search for the minimum of the error function in weight}
\begin{itemize}
\item[\ ]
\[ \textcolor{red}{\frac{\partial E \left( w_{mh} \right) }{ \partial w_{mh} }} = \textcolor{magenta}{\frac{\partial E }{ \partial N_3 }} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} = \textcolor{magenta}{- \delta_h} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} \]
\end{itemize}
\end{block}
\begin{block}{The derivative chain rule}
\begin{itemize}
\item[\ ]
\[ \textcolor{red}{\frac{\partial E \left( w_{mh} \right) }{ \partial w_{mh} }} = \textcolor{magenta}{\frac{\partial E }{ \partial N_3 }} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} = \textcolor{blue}{\frac{ \partial E }{ \partial h }} \cdot \textcolor{orange}{\frac{ \partial h }{ \partial N_3 }} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} \]
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Error backpropagation to the output neuron weights}
\begin{block}{The partial derivative of $E_{MSE}^h$ with respect to the net output $h$}
\begin{itemize}
\item[\ ]
\[ \textcolor{blue}{\frac{ \partial E }{ \partial h }} = \frac{\partial }{\partial h} \cdot \frac{1}{2} \sum_{j \in M} \left(\hat{y}_h - h \right)^2 = -\left(\hat{y}_h - h \right) = \left( h - \hat{y}_h\right)\]
\end{itemize}
\end{block}
\begin{block}{The partial derivative of the sigmoid function }
\begin{itemize}
\item[\ ]
\[ \textcolor{orange}{\frac{ \partial h }{ \partial N_3 }} = \frac{ \partial f(N_3) }{ \partial N_3 } = h(1 - h) \]
\end{itemize}
\end{block}
\begin{block}{The partial derivative of the dot product $N_3 = \langle A_2 W_2 \rangle$}
\begin{itemize}
\item[\ ]
\[ \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} = a_{2m} \]
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Neural Networks: error backpropagation }
\begin{tikzpicture}
  \matrix[mymx] (mx) {
 %          1           &         2        &    3   			&   4     			&   5     			\\  
     |[plain]|   A_1	&[2cm]             & |[plain]| A_2 		&	             	& 	|[plain]| h=A_3	\\[-8mm] %1
	 |[plain]|   b_1  	&             \sum & |[sel]| f(n_{21})	&   |[plain]| b_2 						\\ %2
	 |[plain]|   x_1	&             \sum & |[sel]| f(n_{22})	&   |[psel]| \sum	& |[psel]| f_3(n_{31})\\ %3       
     |[plain]|   x_2    & 			  \sum & |[sel]| f(n_{23})				    						\\ %4
};
  {[route]
    \foreach \y in {2,3,4} {
      \draw (mx-2-2) -- (mx-\y-1);
      \draw (mx-3-2) -- (mx-\y-1);
      \draw (mx-4-2) -- (mx-\y-1);
       }
    \foreach \y in {2,3,4} {
      \draw (mx-\y-3) -- (mx-\y-2); }
    \draw (mx-3-4) -- (mx-2-4); %b_2
    \foreach \y in {2,3,4} {
    \draw (mx-3-4) -- (mx-\y-3);}
    \draw (mx-3-5) -- (mx-3-4);
  }
  {[selroute]
    \draw (mx-3-5) -- (mx-3-4) node[mylabel,above] { e\cdot f_3' };
    \draw (mx-3-4) -- (mx-2-3) node[mylabel] { w_{21} - \vartriangle w };
    \draw (mx-3-4) -- (mx-2-3) node[mylabel,above] { \vartriangle w = e\cdot f_3'\cdot a_{21} };
  }
  \node[above right = 2mm and 11mm of mx-2-1]  {$W_1$};
  \node[above right = 0cm and 2mm of mx-2-3]  {$W_2$};
  \node[above right = 2mm and 0mm of mx-2-2]  {$N_2$};
  \node[above right = 2mm and 0mm of mx-2-4]  {$N_3$};
  \node[above right = 0mm and 0mm of mx-3-5]  {$\displaystyle \textcolor{blue}{\frac{ \partial E }{ \partial h }}=e$};
  \node[below right = 0mm and 0mm of mx-3-5]  {$e=h-y$};
\end{tikzpicture}
\begin{block}{}
\begin{itemize}
\item[\ ] $\displaystyle  \vartriangle w_{mh} = \eta \cdot \textcolor{red}{\frac{\partial E \left( w_{mh} \right) }{ \partial w_{mh} }} = \eta\cdot \textcolor{blue}{\frac{ \partial E }{ \partial h }} \cdot \textcolor{orange}{\frac{ \partial h }{ \partial N_3 }} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} $ \\ 
$\displaystyle  \vartriangle W_{2} = \eta\cdot \left( h - \hat{y}_h\right)\cdot h(1 - h) \cdot A_2  $
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Error backpropagation to the output neuron weights}
\begin{block}{The partial derivative of $E_{MSE}^h$ with respect to the output neuron weights}
\begin{itemize}
\item[\ ]
\[ \textcolor{red}{\frac{\partial E \left( w_{mh} \right) }{ \partial w_{mh} }} = \textcolor{magenta}{- \delta_h} \cdot \textcolor{green}{\frac{ \partial N_3 }{ \partial w_{mh} }} = \textcolor{magenta}{- \delta_h} \cdot a_{2m} = \left( h - \hat{y}_h\right)\cdot h(1 - h) \cdot a_{2m}  \]
\end{itemize}
\end{block}
\begin{block}{Widrow-Hoff rule -- $\delta$-rule}
\begin{itemize}
\item[\ ]
\framebox{$ \vartriangle w_{mh} = \eta \cdot \delta_h \cdot a_{2m} $} \quad \framebox{$ \vartriangle W_{mh} = \eta \cdot \delta_h \cdot A_2 $}
\end{itemize}
\end{block}
\begin{block}{R code}
\begin{itemize}
\item[\ ]
\begin{verbatim}
delta3 = (h - y)*h*(1 - h)
W2 <- W2 - alfa*delta3 %*% t(A2)
\end{verbatim}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Error backpropagation to the hidden neuron weights}
\begin{block}{}
\begin{itemize}
\item[\ ]
\[ \textcolor{red}{\frac{\partial E \left( w_{nm} \right) }{ \partial w_{nm} }} = \textcolor{magenta}{\frac{\partial E }{ \partial N_2 }} \cdot \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} = \textcolor{blue}{\frac{ \partial E }{ \partial A_2 }} \cdot \textcolor{orange}{\frac{ \partial A_2 }{ \partial N_2 }} \cdot \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} \]
\end{itemize}
\end{block}
\begin{block}{The partial derivative of $E_{MSE}^m$ with respect to the hidden layer outputs $A_2$ ($h_1=h$ - it is only one output neuron)}
\begin{itemize}
\item[\ ]  
\[ \textcolor{blue}{\frac{ \partial E }{ \partial A_2 }} = \frac{\partial (E_{h_1} + E_{h_2})}{\partial A_2} = \frac{\partial E^h }{ \partial N_3 } \cdot \frac{ \partial N_3 }{ \partial A_2 } = \textcolor{magenta}{- \delta_h} \cdot W_2 \]
\end{itemize}
\end{block}
\begin{block}{The partial derivative of the sigmoid function }
\begin{itemize}
\item[\ ]
\[ \textcolor{orange}{\frac{ \partial A_2 }{ \partial N_2 }} = \frac{ \partial f(N_2) }{ \partial N_2 } = A_2(1 - A_2) \]
\end{itemize}
\end{block}
\begin{block}{The partial derivative of the dot product $N_2 = \langle A_1 W_1 \rangle$}
\begin{itemize}
\item[\ ]
\[ \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} = a_{1n} \]
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Neural Networks: error backpropagation }
\begin{tikzpicture}
  \matrix[mymx] (mx) {
 %          1           &         2        &    3   			&   4     			&   5     			\\  
     |[plain]|   A_1	&[2cm]             & |[plain]| A_2 		&	             	& 	|[plain]| h=A_3	\\[-8mm] %1
	 |[plain]|   b_1  	&             \sum & |[sel]| f_2(n_{21})	&   |[plain]| b_2 						\\ %2
	 |[plain]|   x_1	&             \sum & |[sel]| f_2(n_{22})	&   |[psel]| \sum	& |[psel]| f_3(n_{31})\\ %3       
     |[plain]|   x_2    & 			  \sum & |[sel]| f_2(n_{23})				    						\\ %4
};
  {[route]
    \foreach \y in {2,3,4} {
      \draw (mx-2-2) -- (mx-\y-1);
      \draw (mx-3-2) -- (mx-\y-1);
      \draw (mx-4-2) -- (mx-\y-1);
       }
    \foreach \y in {2,3,4} {
      \draw (mx-\y-3) -- (mx-\y-2); }
    \draw (mx-3-4) -- (mx-2-4); %b_2
    \foreach \y in {2,3,4} {
    \draw (mx-3-4) -- (mx-\y-3);}
    \draw (mx-3-5) -- (mx-3-4);
  }
  {[selroute]
    \draw (mx-3-5) -- (mx-3-4) node[mylabel,above] { \delta_h=e\cdot f_3' };
    \draw (mx-2-2) -- (mx-2-1) node[mylabel] { w_{11} - \vartriangle w };
    \draw (mx-2-2) -- (mx-2-1) node[mylabel,above] { \vartriangle w = e\cdot f_3'\cdot w_{21} \cdot f_2'\cdot a_{11} };
  }
  \node[above right = 2mm and 11mm of mx-2-1]  {$W_1$};
  \node[above right = 0cm and 2mm of mx-2-3]  {$W_2$};
  \node[above right = 2mm and 0mm of mx-2-2]  {$N_2$};
  \node[above right = 2mm and 0mm of mx-2-4]  {$N_3$};
  \node[above right = 0mm and 0mm of mx-3-5]  {$\displaystyle \textcolor{blue}{\frac{ \partial E }{ \partial h }}=e$};
  \node[below right = 0mm and 0mm of mx-3-5]  {$e=h-y$};
\end{tikzpicture}
\begin{block}{}
\begin{itemize}
\item[\ ] $\displaystyle  \vartriangle w_{nm} = \eta \cdot \textcolor{red}{\frac{\partial E \left( w_{nm} \right) }{ \partial w_{nm} }} = \eta \cdot \textcolor{magenta}{\frac{\partial E }{ \partial N_2 }} \cdot \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} = \eta \cdot \textcolor{blue}{\frac{ \partial E }{ \partial A_2 }} \cdot \textcolor{orange}{\frac{ \partial A_2 }{ \partial N_2 }} \cdot \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} $\\ 
$\displaystyle  \vartriangle W_{1} = \eta \cdot \textcolor{violet}{\delta_m} \cdot A_1 = \textcolor{magenta}{\delta_h} \cdot W_2 \cdot A_2(1 - A_2) \cdot A_1  $
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Error backpropagation to the hidden neuron weights}
\begin{block}{The partial derivative of $E_{MSE}^m$ with respect to the hidden neuron weights}
\begin{itemize}
\item[\ ]
\[ \textcolor{red}{\frac{\partial E \left( w_{nm} \right) }{ \partial w_{nm} }} = \textcolor{violet}{- \delta_m} \cdot \textcolor{green}{\frac{ \partial N_2 }{ \partial w_{nm} }} = \textcolor{violet}{- \delta_m} \cdot a_{1n} = \textcolor{magenta}{- \delta_h} \cdot W_2 \cdot A_2(1 - A_2) \cdot a_{1n}  \]
\end{itemize}
\end{block}
\begin{block}{Widrow-Hoff rule -- $\delta$-rule for the hidden layer}
\begin{itemize}
\item[\ ]
\framebox{$ \vartriangle w_{nm} = \eta \cdot \textcolor{violet}{\delta_m} \cdot a_{1n} = \eta \cdot \textcolor{magenta}{\delta_h} \cdot W_2 \cdot A_2(1 - A_2) \cdot a_{1n}$}\\
$\textcolor{violet}{\delta_m} = \textcolor{magenta}{\delta_h} \cdot W_2 \cdot A_2(1 - A_2)$ \quad \framebox{$ \vartriangle W_{1} = \eta \cdot \textcolor{violet}{\delta_m} \cdot A_1$}
\end{itemize}
\end{block}
\begin{block}{R code}
\begin{itemize}
\item[\ ]
\begin{verbatim}
delta3 = (h - y)*h*(1 - h)
delta2<-(t(W2) %*% delta3 * A2 * (1 - A2))[-1]
W1 <- W1 - alfa*delta2 %*% t(A1)
\end{verbatim}
\end{itemize}
\end{block}
\end{frame}


\end{document}

\begin{frame}
\frametitle{}
\begin{block}{}
\begin{itemize}
\item
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\scriptsize
%\includegraphics[width=11.8cm]{11gdiag\_01.jpg}
\frametitle{}
\begin{lstlisting}
\end{lstlisting}
\end{frame}

