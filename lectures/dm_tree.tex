%lualatex <name>.tex; 
%view <name>.pdf
\documentclass[smaller, proffesionalfonts]{beamer}
\usecolortheme[named=gray]{structure}
\mode<presentation> {
%\usetheme{Madrid} % My favorite!
%\usetheme{Boadilla} % Tretty neat, soft color.
%\usetheme{default}
\usetheme{Warsaw}
%\usetheme{Bergen} % This template has nagivation on the left
%\usetheme{Frankfurt} % Similar to the default with an extra region at the top.
%\usecolortheme{seahorse} % Simple and clean template
%\usetheme{Darmstadt} % not so good
% Uncomment the following line if you want page numbers and using Warsaw theme
% \setbeamertemplate{footline}[page number]
\setbeamercovered{transparent}
%\setbeamercovered{invisible}
% To remove the navigation symbols from the bottom of slides%
\setbeamertemplate{navigation symbols}{} 
}

\usepackage{ifpdf}
\usepackage{ifluatex}
\ifluatex
\usepackage{polyglossia}   
\setmainlanguage{polish}
\usepackage{amsmath}
\def\bm{\boldsymbol}
\else
\usepackage{graphicx}
\usepackage{polski} 
\usepackage[utf8x]{inputenc}
\usepackage{bm} 
\fi
\usepackage{ucs}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{ragged2e}
%\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.
%\apptocmd{\block}{}{\justifying}{} % Allow optional arguments after frame.
% For typesetting bold math (not \mathbold)

\usepackage{pgf,tikz}
\usetikzlibrary{graphdrawing}
\usetikzlibrary{graphs}
\usegdlibrary{trees}

\newcommand{\mycircle}[1]{\tikz{\filldraw[draw=#1,fill=#1] (0,0) circle [radius=0.3em];}}

%\logo{\includegraphics[height=0.6cm]{yourlogo.eps}}
%
\title[EARIN]{Data Mining Lectures - Decision trees}
%
\author{Piotr Wąsiewicz}
\institute[ICS PW]
{
Institute of Computer Science\\
\medskip
{\emph{pwasiewi@elka.pw.edu.pl}}
}
\date{\today}
% \today will show current date. 
% Alternatively, you can specify a date.

\begin{document}
%\lstset{language=SQL}

\lstset{
    language=R,
    inputencoding=utf8x,
    extendedchars=\true,
    literate={ą}{{\k{a}}}1
             {Ą}{{\k{A}}}1
             {ę}{{\k{e}}}1
             {Ę}{{\k{E}}}1
             {ó}{{\'o}}1
             {Ó}{{\'O}}1
             {ś}{{\'s}}1
             {Ś}{{\'S}}1
             {ł}{{\l{}}}1
             {Ł}{{\L{}}}1
             {ż}{{\.z}}1
             {Ż}{{\.Z}}1
             {ź}{{\'z}}1
             {Ź}{{\'Z}}1
             {ć}{{\'c}}1
             {Ć}{{\'C}}1
             {ń}{{\'n}}1
             {Ń}{{\'N}}1
}
%\lstset{language=c}
%\lstset{backgroundcolor=\color{brown}}
%\lstset{linewidth=90mm}
%\lstset{frameround=tttt}
\lstset{frameround=trbl}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\textit, stringstyle=\upshape,showspaces=false}
%\lstset{commentstyle=\textit}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\small
\frametitle{Decision trees}
\begin{block}{}
\begin{itemize}
\item 
\justifying
A hierarchical structure representing dataset/domain partitioning nodes: splits based on attribute-value conditions and leaves: class labels or probability distributions.
\item Prediction by descending the tree at each node dispatching along a branch at each leaf a class label or probability determined.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{}
\begin{block}{Splits for discrete attributes}
\begin{itemize}
\item Value-based: split outcomes correspond to single attribute values.
\item Equality based: split outcomes correspond to binary equality test results. 
\item Partition-based: split outcomes correspond to attribute value subsets.
\item Membership-based: split outcomes correspond to binary membership test results.
\end{itemize}
\end{block}
\begin{block}{Splits for numeric attributes}
\begin{itemize}
\item Inequality based: split outcomes correspond to binary inequality test results.
\item Interval-based: split outcomes correspond to attribute value interval.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Decision tree growing}
\begin{block}{}
\begin{enumerate}
\item create the root node and mark it as open;
\item assign all training instances from T to the root node;
\item while there are open nodes:
\begin{itemize}
\item[A.] select an open node n;
\item[B.] calculate class distribution $T(d|n)$ based on T[n];
\item[C.] assign class label $argmax[d]T(d|n)$ to n;
\item[D.] if stop criteria are satisfied for n mark n as a closed leaf; else
\begin{enumerate}
\item select a split t for n; 
\item for each outcome r of split t:
\item[A.] create a descendant node n[r] corresponding to r and mark it as open;
\item[B.] assign all instances from T[n,t=r] to n[r];
\item[C.] mark n as a closed node;
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}
\frametitle{}
\begin{block}{Stop criteria}
\begin{itemize}
\item Uniform class: all training instances in the node are of the same class.
\item No instances left: the set of training instances assigned to the node is empty.
\item No splits left: there is no split that can be applied to further partition the current subset of training instances.
\item Can be relaxed:
\begin{enumerate}
\item most instances of the same class (low class impurity),
\item less than a specified minimum number of instances,
\item the best available split is not sufficiently good.
\end{enumerate}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{}
\begin{block}{Split selection}
\begin{itemize}
\item Strict stop criteria guarantee training set error minimization.
\item Split selection responsible for overfitting avoidance.
\item Ockham's razor: among trees with the same training set error prefer smaller ones and it can be achieved by minimizing class impurity e.g. its entropy.
\end{itemize}
\end{block}
\begin{block}{Pruning and probability classification}
\begin{itemize}
\item In pruning the complexity parameter (cp) controls the tradeoff between error and size
\item Class probability distribution at leaves enables probabilistic prediction
\item Can be used to minimize misclassification costs; instead of predicting the most probable class predict the class with the minimum expected cost,
\item Can be used to adjust the operating point for binary classification e.g. obtaining the ROC curve
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{An exemplary test dataset $T$}
\begin{itemize}
\item
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $a_1$ & $b_1$ & $c_1$ & 1 & $\bm 1$ \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & 1 & $\bm 1$ \\
   \hline
    $a_1$ & $b_2$ & $c_1$ & 0 & $\bm 1$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & $\bm 1$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & $\bm 1$ \\
   \hline
   \end{tabular}
}
\end{center}
\item
\justifying
The dataset $T$ has three input attributes $a,b,c$ and target class labels $C$ and a nodemap $S$ - current tree terminal node indices which are attached the given instances to.
At the beginning of creating (growing) the tree we have only one node with a number 1 and it is a root and a temporary leaf simultaneously, so the nodemap has only one node.

\begin{center}
\begin{tikzpicture}
	\node[draw, circle]  {$\bm 1$};
\end{tikzpicture}
\end{center}

\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{The impurity measure of splits}
\begin{itemize}
\item
In order to select a split condition first the impurity measure for all atrribute value splits e.g. $a(x)=a_1$ are calculated based on entropy e.g. for $a(x)=a_1$:
$E_{a,a_1}(T)= 
-\frac{|T^{0}_{a,a_1}|}{|T_{a,a_1}|}\log_2(\frac{|T^{0}_{a,a_1}|}{|T_{a,a_1}|}) 
-\frac{|T^{1}_{a,a_1}|}{|T_{a,a_1}|}\log_2(\frac{|T^{1}_{a,a_1}|}{|T_{a,a_1}|}) =
-\frac{1}{2}\log_2(\frac{1}{2})-\frac{1}{2}\log_2(\frac{1}{2})=1$
\item 
\item
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $\bm a_1$ & $b_1$ & $c_1$ & $\bm 1$ & 1 \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & 1 & 1 \\
   \hline
    $\bm a_1$ & $b_2$ & $c_1$ & $\bm 0$ & 1 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & 1 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & 1 \\
   \hline
   \end{tabular}
}
\end{center}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Creating new nodes based on the selected split - disjoint instances subsets}
\begin{itemize}
\item
\justifying
The split with the smallest entropy equal to 0 or as close as possible to 0 is selected. If several entropies are equal to 0, the split e.g. with the largest number of instances is chosen: $a(x)=a_2$ has three instances with the same class 1 this means $C_{a(x)=a_2}=1$ or $C(a_2)_1=True$ and stop criteria for the same target class values are applied successfully. 
\item 
$E_{a,a_2}(T)= 
-\frac{|T^{0}_{a,a_2}|}{|T_{a,a_2}|}\log_2(\frac{|T^{0}_{a,a_2}|}{|T_{a,a_2}|}) 
-\frac{|T^{1}_{a,a_2}|}{|T_{a,a_2}|}\log_2(\frac{|T^{1}_{a,a_2}|}{|T_{a,a_2}|}) =
-\frac{0}{3}\log_2(\frac{0}{3})-\frac{3}{3}\log_2(\frac{3}{3})=0$
\item
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$T$],  
			3 [label=$F$]
		} 
    };
\end{tikzpicture}
\hfill
\end{minipage}
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$C_{0,1}$]
		} 
    };
\end{tikzpicture}
\end{minipage}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Creating a leaf after the stop criterion is satisfied}
\begin{itemize}
\item
\justifying
Based on the chosen split condition $a(x)=a_2$ the new nodemap is generated. The instances with the following value attributes $a(x)=a_2$ belong to the branch of the node 2 and as it was earlier mentioned the node 2 becomes the tree leaf (a terminal node with one class value) and the rest of instances belongs to the node 3. Thus, not necessary all target classes in the branch subsets should be equal to 1, sometimes the majority of the same instances class values above the given threshold is the admisible criterion for creating leaves. 
\item
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $a_1$ & $b_1$ & $c_1$ & 1 & 3 \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & $\bm 1$ & $\bm 2$ \\
   \hline
    $a_1$ & $b_2$ & $c_1$ & 0 & 3 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & $\bm 1$ & $\bm 2$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & $\bm 1$ & $\bm 2$ \\
   \hline
   \end{tabular}
}
\end{center}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{The next layer of child binary tree nodes: $2n$ and $2n+1$ }
\begin{itemize}
\item
\justifying
In the next steps for every next node instance subset the split with the smallest entropy equal to 0 or as close as possible to 0 is also chosen. 
\item
But for the node 2 instances denoted in the nodemap the stop criterion is met and it can become a leaf which accurately classifies the attached test dataset instances and hopefully will achieve good accuracy for new similar instances chosen by the condition $a(x)=a_2$ (there will be a few false positives).
\item 
For the node 3 branch instance subset the attribute $b$ values are appriopriate: for each target class it has different values. It creates two nodes with binary tree child nodes numbering $2n=6$ and $2n+1=7$.

$E_{b,b_1}(T)= 
-\frac{|T^{0}_{b,b_1}|}{|T_{b,b_1}|}\log_2(\frac{|T^{0}_{b,b_1}|}{|T_{b,b_1}|}) 
-\frac{|T^{1}_{b,b_1}|}{|T_{b,b_1}|}\log_2(\frac{|T^{1}_{b,b_1}|}{|T_{b,b_1}|}) =
-\frac{0}{1}\log_2(\frac{0}{1})-\frac{1}{1}\log_2(\frac{1}{1})=0$

$E_{b,b_2}(T)=0$

$E_{c,c_1}(T)=-\frac{1}{2}\log_2(\frac{1}{2})-\frac{1}{2}\log_2(\frac{1}{2})=1$

\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Visualization of the final classifier generation}
\begin{itemize}
\item[\ ]
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$T_{b_1}$] -> { 6 [label=$T$], 7 [label=$F$]}
		} 
    };
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$T_{b_1}$] -> { 6 [label=$C_1$], 7 [label=$C_0$]}
		} 
    };
\end{tikzpicture}
\end{minipage}
\end{itemize}
\end{block}
\end{frame}

% End of slides
\end{document} 


\begin{frame}
\frametitle{}
\begin{block}{}
\begin{itemize}
\item
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\scriptsize
\frametitle{}
\begin{lstlisting}
\end{lstlisting}
\end{frame}

\begin{frame}
%\frametitle{}
%\includegraphics[width=11.8cm]{11gdiag\_01.jpg}
\small
\begin{block}{}
\begin{itemize}
\item 
\end{itemize}
\end{block}
\end{frame}

