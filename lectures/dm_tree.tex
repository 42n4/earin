%lualatex <name>.tex; 
%view <name>.pdf
\documentclass[smaller, proffesionalfonts]{beamer}
\usecolortheme[named=gray]{structure}
\mode<presentation> {
%\usetheme{Madrid} % My favorite!
%\usetheme{Boadilla} % Tretty neat, soft color.
%\usetheme{default}
\usetheme{Warsaw}
%\usetheme{Bergen} % This template has nagivation on the left
%\usetheme{Frankfurt} % Similar to the default with an extra region at the top.
%\usecolortheme{seahorse} % Simple and clean template
%\usetheme{Darmstadt} % not so good
% Uncomment the following line if you want page numbers and using Warsaw theme
% \setbeamertemplate{footline}[page number]
\setbeamercovered{transparent}
%\setbeamercovered{invisible}
% To remove the navigation symbols from the bottom of slides%
\setbeamertemplate{navigation symbols}{} 
}

\usepackage{ifpdf}
\usepackage{ifluatex}
\ifluatex
\usepackage{polyglossia}   
\setmainlanguage{polish}
\usepackage{amsmath}
\def\bm{\boldsymbol}
\usepackage{lualatex-math}
\else
\usepackage{graphicx}
\usepackage{polski} 
\usepackage[utf8x]{inputenc}
\usepackage{bm} 
\fi
\usepackage{ucs}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{ragged2e}
%\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.
%\apptocmd{\block}{}{\justifying}{} % Allow optional arguments after frame.
% For typesetting bold math (not \mathbold)

\usepackage{pgf,tikz}
\usetikzlibrary{graphdrawing}
\usetikzlibrary{graphs}
\usegdlibrary{trees}
\usepackage{pgfplots}

\newcommand{\mycircle}[1]{\tikz{\filldraw[draw=#1,fill=#1] (0,0) circle [radius=0.3em];}}

%\logo{\includegraphics[height=0.6cm]{yourlogo.eps}}
%
\title[EARIN]{Data Mining Lectures - Decision trees}
%
\author{Piotr Wąsiewicz}
\institute[ICS PW]
{
Institute of Computer Science\\
\medskip
{\emph{pwasiewi@elka.pw.edu.pl}}
}
\date{\today}
% \today will show current date. 
% Alternatively, you can specify a date.

\begin{document}
%\lstset{language=SQL}

\lstset{
    language=R,
    inputencoding=utf8x,
    extendedchars=\true,
    literate={ą}{{\k{a}}}1
             {Ą}{{\k{A}}}1
             {ę}{{\k{e}}}1
             {Ę}{{\k{E}}}1
             {ó}{{\'o}}1
             {Ó}{{\'O}}1
             {ś}{{\'s}}1
             {Ś}{{\'S}}1
             {ł}{{\l{}}}1
             {Ł}{{\L{}}}1
             {ż}{{\.z}}1
             {Ż}{{\.Z}}1
             {ź}{{\'z}}1
             {Ź}{{\'Z}}1
             {ć}{{\'c}}1
             {Ć}{{\'C}}1
             {ń}{{\'n}}1
             {Ń}{{\'N}}1
}
%\lstset{language=c}
%\lstset{backgroundcolor=\color{brown}}
%\lstset{linewidth=90mm}
%\lstset{frameround=tttt}
%\lstset{frameround=trbl}
%\lstset{keywordstyle=\color{black}\bfseries}
%\lstset{commentstyle=\textit, stringstyle=\upshape,showspaces=false}
%\lstset{commentstyle=\textit}

\newcommand{\listingsfont}{\ttfamily}



\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\small
\frametitle{Decision trees}
\begin{block}{}
\begin{itemize}
\item 
\justifying
A hierarchical structure representing dataset/domain partitioning nodes: splits based on attribute-value conditions and leaves: class labels or probability distributions.
\item Prediction by descending the tree at each node dispatching along a branch at each leaf a class label or probability determined.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{}
\begin{block}{Splits for discrete attributes}
\begin{itemize}
\item Value-based: split outcomes correspond to single attribute values.
\item Equality based: split outcomes correspond to binary equality test results. 
\item Partition-based: split outcomes correspond to attribute value subsets.
\item Membership-based: split outcomes correspond to binary membership test results.
\end{itemize}
\end{block}
\begin{block}{Splits for numeric attributes}
\begin{itemize}
\item Inequality based: split outcomes correspond to binary inequality test results.
\item Interval-based: split outcomes correspond to attribute value interval.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Decision tree growing}
\begin{block}{}
\begin{enumerate}
\item create the root node and mark it as open;
\item assign all training instances from T to the root node;
\item while there are open nodes:
\begin{itemize}
\item[A.] select an open node n;
\item[B.] calculate class distribution $T(d|n)$ based on T[n];
\item[C.] assign class label $argmax[d]T(d|n)$ to n;
\item[D.] if stop criteria are satisfied for n mark n as a closed leaf; else
\begin{enumerate}
\item select a split t for n; 
\item for each outcome r of split t:
\item[A.] create a descendant node n[r] corresponding to r and mark it as open;
\item[B.] assign all instances from T[n,t=r] to n[r];
\item[C.] mark n as a closed node;
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}
\frametitle{}
\begin{block}{Stop criteria}
\begin{itemize}
\item Uniform class: all training instances in the node are of the same class.
\item No instances left: the set of training instances assigned to the node is empty.
\item No splits left: there is no split that can be applied to further partition the current subset of training instances.
\item Can be relaxed:
\begin{enumerate}
\item most instances of the same class (low class impurity),
\item less than a specified minimum number of instances,
\item the best available split is not sufficiently good.
\end{enumerate}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{}
\begin{block}{Split selection}
\begin{itemize}
\item Strict stop criteria guarantee training set error minimization.
\item Split selection responsible for overfitting avoidance.
\item Ockham's razor: among trees with the same training set error prefer smaller ones and it can be achieved by minimizing class impurity e.g. its entropy.
\end{itemize}
\end{block}
\begin{block}{Pruning and probability classification}
\begin{itemize}
\item In pruning the complexity parameter (cp) controls the tradeoff between error and size
\item Class probability distribution at leaves enables probabilistic prediction
\item Can be used to minimize misclassification costs; instead of predicting the most probable class predict the class with the minimum expected cost,
\item Can be used to adjust the operating point for binary classification e.g. obtaining the ROC curve
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{An exemplary test dataset $T$}
\begin{itemize}
\item[\ ]
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $a_1$ & $b_1$ & $c_1$ & 1 & $\bm 1$ \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & 1 & $\bm 1$ \\
   \hline
    $a_1$ & $b_2$ & $c_1$ & 0 & $\bm 1$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & $\bm 1$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & $\bm 1$ \\
   \hline
   \end{tabular}
}
\end{center}
\item
\justifying
The dataset $T$ has three input attributes $a,b,c$ and target class labels $C$ and a nodemap $S$ - current tree terminal node indices which are attached the given instances to.
At the beginning of creating (growing) the tree we have only one node with a number 1 and it is a root and also a temporary leaf, so the nodemap has only one node.

\begin{center}
\begin{tikzpicture}
	\node[draw, circle]  {$\bm 1$};
\end{tikzpicture}
\end{center}

\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{The impurity measure of splits}
\begin{itemize}
\item
In order to select a split condition first the impurity measure for all atrribute value splits e.g. $a(x)=a_1$ are calculated based on entropy e.g. for $a(x)=a_1$:
$E_{a,a_1}(T)= 
-\frac{|T^{0}_{a,a_1}|}{|T_{a,a_1}|}\log_2(\frac{|T^{0}_{a,a_1}|}{|T_{a,a_1}|}) 
-\frac{|T^{1}_{a,a_1}|}{|T_{a,a_1}|}\log_2(\frac{|T^{1}_{a,a_1}|}{|T_{a,a_1}|}) =
-\frac{1}{2}\log_2(\frac{1}{2})-\frac{1}{2}\log_2(\frac{1}{2})=1$
\item[\ ]
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $\bm a_1$ & $b_1$ & $c_1$ & $\bm 1$ & 1 \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & 1 & 1 \\
   \hline
    $\bm a_1$ & $b_2$ & $c_1$ & $\bm 0$ & 1 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & 1 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & 1 & 1 \\
   \hline
   \end{tabular}
}
\end{center}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Creating new nodes based on the selected split - disjoint instances subsets}
\begin{itemize}
\item
\justifying
The split with the smallest entropy equal to 0 or as close as possible to 0 is selected. If several entropies are equal to 0, the split e.g. with the largest number of instances is chosen: $a(x)=a_2$ has three instances with the same class 1 this means $C_{a(x)=a_2}=1$ or $C(a_2)_1=True$ and stop criteria for the same target class values are applied successfully. 
\item 
$E_{a,a_2}(T)= 
-\frac{|T^{0}_{a,a_2}|}{|T_{a,a_2}|}\log_2(\frac{|T^{0}_{a,a_2}|}{|T_{a,a_2}|}) 
-\frac{|T^{1}_{a,a_2}|}{|T_{a,a_2}|}\log_2(\frac{|T^{1}_{a,a_2}|}{|T_{a,a_2}|}) =
-\frac{0}{3}\log_2(\frac{0}{3})-\frac{3}{3}\log_2(\frac{3}{3})=0$
\item[\ ]
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$T$],  
			3 [label=$F$]
		} 
    };
\end{tikzpicture}
\hfill
\end{minipage}
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$C_{0,1}$]
		} 
    };
\end{tikzpicture}
\end{minipage}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Creating a leaf after the stop criterion is satisfied}
\begin{itemize}
\item
\justifying
Based on the chosen split condition $a(x)=a_2$ the new nodemap is generated. The instances with the following value attributes $a(x)=a_2$ belong to the branch of the node 2 and as it was earlier mentioned the node 2 becomes the tree leaf (a terminal node with one class value) and the rest of instances belongs to the node 3. Thus, it is not necessary that all target classes in the branch subsets should be equal only to one class, sometimes the majority of the instances with the same class above the given threshold is the admisible criterion for creating leaves. 
\item[\ ]
\begin{center}
{\sf
   \begin{tabular}{|p{3mm}||c|c||c||c|}
   \hline 
   a & b & c & class (C) & nodemap (S)\\
   \hline
   \hline
    $a_1$ & $b_1$ & $c_1$ & 1 & 3 \\
   \hline
    $a_2$ & $b_1$ & $c_1$ & $\bm 1$ & $\bm 2$ \\
   \hline
    $a_1$ & $b_2$ & $c_1$ & 0 & 3 \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & $\bm 1$ & $\bm 2$ \\
   \hline
    $a_2$ & $b_2$ & $c_2$ & $\bm 1$ & $\bm 2$ \\
   \hline
   \end{tabular}
}
\end{center}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{The next layer of child binary tree nodes: $2n$ and $2n+1$ }
\begin{itemize}
\item
\justifying
In the next steps for every next node instance subset the split with the smallest entropy equal to 0 or as close as possible to 0 is also chosen. 
\item
But for the node 2 instances denoted in the nodemap the stop criterion is met and it can become a leaf which accurately classifies the attached test dataset instances and hopefully will achieve good accuracy for new similar instances chosen by the condition $a(x)=a_2$.
\item 
For the node 3 instance subset the attribute $b$ values are appriopriate: for each target class it has different values with entropy equal to 0. It creates two binary child nodes numbered $2n=6$ and $2n+1=7$ (only for binary trees).

$E_{b,b_1}(T)= 
-\frac{|T^{0}_{b,b_1}|}{|T_{b,b_1}|}\log_2(\frac{|T^{0}_{b,b_1}|}{|T_{b,b_1}|}) 
-\frac{|T^{1}_{b,b_1}|}{|T_{b,b_1}|}\log_2(\frac{|T^{1}_{b,b_1}|}{|T_{b,b_1}|}) =
-\frac{0}{1}\log_2(\frac{0}{1})-\frac{1}{1}\log_2(\frac{1}{1})=0$

$E_{b,b_2}(T)=0$

$E_{c,c_1}(T)=-\frac{1}{2}\log_2(\frac{1}{2})-\frac{1}{2}\log_2(\frac{1}{2})=1$

\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Growing the exemplary decision tree}
\begin{block}{Visualization of the final classifier generation}
\begin{itemize}
\item[\ ]
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$T_{b_1}$] -> { 6 [label=$T$], 7 [label=$F$]}
		} 
    };
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
\begin{tikzpicture}[>=stealth, every node/.style={circle, draw, minimum size=0.75cm}]
\graph [tree layout, grow=down, fresh nodes, level distance=0.5in, sibling distance=0.5in]
    {
        1 [label=$T_{a_2}$] -> { 
			 2 [label=$C_1$],  
			3 [label=$T_{b_1}$] -> { 6 [label=$C_1$], 7 [label=$C_0$]}
		} 
    };
\end{tikzpicture}
\end{minipage}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{Confusion Matrix}
\begin{block}{Positives and Negatives}
\begin{itemize}
\item[\ ] 
rci <- runif(nrow(census))
\item[\ ] 
ci.train <- census[rci>=0.33,]
\item[\ ] 
ci.val <- census[rci<0.33,]
\item[\ ] 
ci.tree.d <- rpart(income~., ci.train)
\item[\ ] 
ci.tree.d.pred <- predict(ci.tree.d, ci.val, type="c")
\item[\ ] 
ci.tree.d.cm <- confmat(ci.tree.d.pred, ci.val\$income)
\item[\ ] 
\begin{center}
{\sf
   \begin{tabular}{|c||c|c|}
   \hline 
    & true 0 & true 1 \\
   \hline
   \hline
    pred 0 & True Negative & False Negative (II error) \\
     & TN & FN \\
   \hline
    pred 1 & False Positive (I error) & True Positive \\
    & FP & TP \\
   \hline
   \end{tabular}
}
\end{center}
\item[\ ] FN is much worse than FP e.g. not knowing about cancer is much worse than to be informed that cancer is detected and it is not really present
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Confusion Matrix}
\begin{block}{Performance measures}
\begin{itemize}
\item
$\frac{FP+FN}{TP+TN+FP+FN}$
misclassification error
\item
$\frac{TP+TN}{TP+TN+FP+FN}$
accuracy
\item
$\frac{TP}{TP+FN}$
true positive rate {recall, sensitivity} -
the ratio of correctly classified as positive to all positive
\item
$\frac{FP}{TN+FP}$
false positive rate -
the ratio of incorrectly classified as positive to all negative
\item
$\frac{TP}{TP+FP}$
precision -
the ratio of correctly classified as positive to all instances classified as positive 
\item
$\frac{TN}{TN+FP}$
specificity {1 - fpr} -
the ratio of correctly classified as negatives to all negative instances 
\item
Complementary pairs for detecting e.g. trivial classifiers which predicts only one class:
\begin{itemize}
\item
tpr - fpr
\item
precision - recall
\item
sensitivity - specificity
\end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Confusion Matrix}
\begin{block}{Harmonic mean as a compromise between maximizing precision and recall}
\begin{itemize}
\item F-measure = $\frac{1}{\frac{\frac{1}{precision}+\frac{1}{recall}}{2}}$
\item Code in R: function(cm) { 1/mean(c(1/precision(cm), 1/recall(cm))) }
\end{itemize}
\end{block}
\begin{block}{Handling more than two classes}
\begin{itemize}
\item $1-vs-1$ one class is positive and another one is negative
\item $1-vs-rest$ one class is positive the remaining classes are negative: confmat01 - function in R
\item[\ ]
cm.multi<-confmat01(ci.tree.d.pred.multi, ci.val\$multiclass)
\item[\ ]
rowMeans(sapply(cm.multi,function(cm) c(tpr=tpr(cm), fpr=fpr(cm), fm=f.measure(cm))))
\end{itemize}
\end{block}
\begin{block}{Weighted confusion matrix}
\begin{itemize}
\item For instances with weights: wconfmat
\item tpr and fpr are the same for weighted and unweighted matrices if the same weights are assigned to one class instances.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\small
\frametitle{Receiver operating characteristics (ROC)}
\begin{block}{}
\begin{itemize}
\item
In order to obtain ROC characteristics it is a need for the classifier label output with probabilities of positive class 1, if you have a label domain with two values: 0 and 1.  
\item[\ ]
\begin{tikzpicture}
\begin{axis}[xlabel=fpr,ylabel=tpr,xmax=1,ymax=1,samples=50]
  \addplot[blue, ultra thick] coordinates {(0,0) (0,1)};
  \addlegendentry{All instances correctly labeled}
  \addplot[blue, ultra thick] coordinates {(0,1) (1,1)};
  \addlegendentry{Ideal ROC}
  \addplot[red,  ultra thick][domain=0:1] (x,x);
  \addlegendentry{Random model regression line}
\end{axis}
\end{tikzpicture}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{ROC features}
\begin{block}{}
\begin{itemize}
\item 
All operating points should be above the red line: tpr should be higher than fpr or in the ideal case the only operating point should be (0,1) - the blue line.
\item 
(1,0) - the worst operating point, (1,1) - the classifier always predicts class 1, (0,0) - the classifier always predicts class 0
\item 
The output probabilities for the positive class are sorted to make a ROC curve.
\item
For each value of probability treated as a threshold for being the positive class (above the threshold) or the negative class (below the threshold) starting from the highest probability the all TP and FP are counted resulting in tpr and fpr rates, which give operating points and finally after joining them with lines - the ROC curve.
\item
The default optimal operating point is for the threshold equal to 0.5, but it is often not the best point.
\item
Moving along the ROC curve shifting the operating point is possible by changing the mentioned threshold.
\end{itemize}
\end{block}
\end{frame}


% End of slides
\end{document} 


\begin{frame}
\frametitle{}
\begin{block}{}
\begin{itemize}
\item
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\scriptsize
\frametitle{}
\begin{lstlisting}
\end{lstlisting}
\end{frame}

\begin{frame}
%\frametitle{}
%\includegraphics[width=11.8cm]{11gdiag\_01.jpg}
\small
\begin{block}{}
\begin{itemize}
\item 
\end{itemize}
\end{block}
\end{frame}

